{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobiML GeoTrackNet Demo\n",
    "\n",
    "Based on: https://github.com/CIA-Oceanix/GeoTrackNet (MIT Licensed, (c) 2018 Duong Nguyen)\n",
    "\n",
    "As presented in Nguyen, D., Vadaine, R., Hajduch, G., Garello, R. (2022). GeoTrackNet - A Maritime Anomaly Detector Using Probabilistic Neural Network Representation of AIS Tracks and A Contrario Detection. In IEEE Transactions on Intelligent Transportation Systems, 23(6).\n",
    "\n",
    "\n",
    "Using data from AISDK: http://web.ais.dk/aisdata/aisdk-2018-02.zip\n",
    "\n",
    "*It is possible to further explore maritime traffic patterns with the TrAISformer (https://github.com/CIA-Oceanix/TrAISformer), which is used for vessel trajectory prediction. The TrAISformer can be trained with AIS data and the preprocessing steps are similar to those of GeoTrackNet. However, the TrAISformer is out of the scope of MobiML and is an optional extension for the user to explore.*\n",
    "\n",
    "## Environments\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "It is recommended to perform the preprocessing steps with the MobiML environment.\n",
    "\n",
    "### Model Training\n",
    "\n",
    "Set up a dedicated GeoTrackNet environment (PY3GPU) to train the model as instructed by Nguyen et al. (2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import movingpandas as mpd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from mobiml.datasets import AISDK\n",
    "from mobiml.preprocessing import (\n",
    "    TrajectorySplitter,\n",
    "    TrajectoryFilter,\n",
    "    TrajectorySubsampler,\n",
    ")\n",
    "from mobiml.transforms import TemporalSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AISDK dataset\n",
    "LAT, LON, SOG, COG, NAME, SHIPTYPE, NAV_STT, TIMESTAMP, TRAJ_ID = list(range(9))\n",
    "\n",
    "EPOCH = datetime(1970, 1, 1)\n",
    "\n",
    "SOG_MIN = 2.0\n",
    "SOG_MAX = 30.0  # SOG is truncated to 30.0 knots max\n",
    "\n",
    "# Pkl filenames\n",
    "pkl_filename_train = \"aisdk_20180208_train.pkl\"\n",
    "pkl_filename_valid = \"aisdk_20180208_valid.pkl\"\n",
    "pkl_filename_test = \"aisdk_20180208_test.pkl\"\n",
    "\n",
    "# Path to csv files\n",
    "data_path = \"data/aisdk_20180208_sample/\"\n",
    "csv_filename = \"aisdk_20180208_sample.csv\"\n",
    "\n",
    "# Output path\n",
    "out_path = \"data/aisdk_20180208_sample/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set up bounding box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(data_path, csv_filename)\n",
    "print(f\"{datetime.now()} Loading data from {path}\")\n",
    "aisdk = AISDK(path)  # you can specify a bounding box here to filter the area\n",
    "LON_MIN, LAT_MIN, LON_MAX, LAT_MAX = aisdk.get_bounds()\n",
    "print(\n",
    "    f\"Bounding box:\\nmin_lon: {LON_MIN}\\nmin_lat: {LAT_MIN}\\nmax_lon: {LON_MAX}\\nmax_lat: {LAT_MAX}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk.df = aisdk.df.dropna()\n",
    "aisdk.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After removing missing values we have...\")\n",
    "print(\"Total number of AIS messages: \", aisdk.df.shape[0])\n",
    "print(\"Total number of vessels:\", len(aisdk.df.traj_id.unique()))\n",
    "print(\"Lat min: \", aisdk.df.y.min(), \"Lat max: \", aisdk.df.y.max())\n",
    "print(\"Lon min: \", aisdk.df.x.min(), \"Lon max: \", aisdk.df.x.max())\n",
    "print(\"Time min: \", aisdk.df.timestamp.min(), \"Time max: \", aisdk.df.timestamp.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove 'Moored' and 'At anchor' AIS messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk.df.drop(aisdk.df[(aisdk.df[\"nav_status\"] == \"Moored\") | (aisdk.df[\"nav_status\"] == \"At anchor\")].index, inplace=True)\n",
    "print(\"After removing 'Moored' or 'At anchor' AIS messages we have...\")\n",
    "print(\"Total number of AIS messages: \", aisdk.df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep only 'Cargo', 'Tanker', 'Passenger' vessel types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk.df = aisdk.df[\n",
    "    (aisdk.df[\"ship_type\"] == \"Cargo\")\n",
    "    | (aisdk.df[\"ship_type\"] == \"Tanker\")\n",
    "    | (aisdk.df[\"ship_type\"] == \"Passenger\")\n",
    "]\n",
    "print(\"After keeping only 'Cargo', 'Tanker' or 'Passenger' AIS messages we have...\")\n",
    "print(\"Total number of AIS messages: \", aisdk.df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split trajectories with observation gaps > 2 hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk = TrajectorySplitter(aisdk).split(observation_gap=timedelta(hours=2))\n",
    "print(\"After splitting trajectories with observation gaps we have...\")\n",
    "print(\"Total number of AIS messages: \", aisdk.df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop trajectories with fewer than $Points_{min}$ locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk = TrajectoryFilter(aisdk).filter_min_pts(min_pts=20)\n",
    "print(\"After removing trajectories with too few points we have...\")\n",
    "print(\"Total number of AIS messages: \", aisdk.df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop speed outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk = TrajectoryFilter(aisdk).filter_speed(min_speed=SOG_MIN, max_speed=SOG_MAX)\n",
    "print(\"After removing speed outliers by setting a minimum and maximum speed we have...\")\n",
    "print(\"Total number of AIS messages: \", aisdk.df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = aisdk.to_trajs() #  mpd.TrajectoryCollection(aisdk.df, \"traj_id\", t=\"timestamp\", x=\"x\", y=\"y\")\n",
    "traj_gdf = tc.to_traj_gdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also want to remove trajectories based on their overall average speed rather than the SOG values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in traj_gdf.iterrows():\n",
    "    traj_gdf.loc[index, \"speed_ok\"] = (\n",
    "        tc.trajectories[index].get_length()\n",
    "        / tc.trajectories[index].get_duration().total_seconds()\n",
    "        > 1.02889  # 2 knots\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_gdf = traj_gdf[traj_gdf[\"speed_ok\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk.df = pd.merge(aisdk.df, traj_gdf[\"traj_id\"], how=\"inner\")\n",
    "print(\"After removing speed outliers based on length and duration we have...\")\n",
    "print(\"Total number of AIS messages: \", aisdk.df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data preparation\n",
    "#### Subsample AIS tracks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk = TrajectorySubsampler(aisdk).subsample(min_dt_sec=60)\n",
    "print(\"After subsampling AIS tracks we have...\")\n",
    "print(\"Total number of AIS messages: \", aisdk.df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal train/valid/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk = TemporalSplitter(aisdk).split_hr()\n",
    "aisdk.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk_train = aisdk.df[(aisdk.df[\"split\"] == 1.0)]\n",
    "aisdk_valid = aisdk.df[(aisdk.df[\"split\"] == 2.0)]\n",
    "aisdk_test = aisdk.df[(aisdk.df[\"split\"] == 3.0)]\n",
    "\n",
    "print(\"Total number of AIS messages: \", len(aisdk.df))\n",
    "print(\"Number of msgs in the training set: \", len(aisdk_train))\n",
    "print(\"Number of msgs in the validation set: \", len(aisdk_valid))\n",
    "print(\"Number of msgs in the test set: \", len(aisdk_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column_order=[\"y\", \"x\", \"speed\", \"direction\", \"Name\", \"ship_type\", \"nav_status\", \"timestamp\", \"traj_id\"]\n",
    "aisdk_train = aisdk_train[target_column_order].reset_index(drop=True)\n",
    "aisdk_valid = aisdk_valid[target_column_order].reset_index(drop=True)\n",
    "aisdk_test = aisdk_test[target_column_order].reset_index(drop=True)\n",
    "aisdk_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk_train[\"timestamp\"] = (aisdk_train[\"timestamp\"].astype(int) / 1_000_000_000).astype(int)\n",
    "aisdk_valid[\"timestamp\"] = (aisdk_valid[\"timestamp\"].astype(int) / 1_000_000_000).astype(int)\n",
    "aisdk_test[\"timestamp\"]  = (aisdk_test[\"timestamp\"].astype(int) / 1_000_000_000).astype(int)\n",
    "aisdk_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format to ndarrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk_train = np.array(aisdk_train)\n",
    "aisdk_valid = np.array(aisdk_valid)\n",
    "aisdk_test = np.array(aisdk_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging into dict\n",
    "Creating AIS tracks from the list of AIS messages. Each AIS track is formatted by a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Convert to dicts of vessel's tracks...\")\n",
    "\n",
    "def convert_tracks_to_dicts(tracks):\n",
    "    d = dict()\n",
    "    for v_msg in tqdm(tracks):\n",
    "        mmsi = int(v_msg[TRAJ_ID])\n",
    "        if not (mmsi in list(d.keys())):\n",
    "            d[mmsi] = np.empty((0, 9))\n",
    "        d[mmsi] = np.concatenate(\n",
    "            (d[mmsi], np.expand_dims(v_msg[:9], 0)), axis=0\n",
    "        )\n",
    "    for key in tqdm(list(d.keys())):\n",
    "        d[key] = np.array(\n",
    "            sorted(d[key], key=lambda m_entry: m_entry[TIMESTAMP])\n",
    "        )\n",
    "    return d\n",
    "\n",
    "Vs_train = convert_tracks_to_dicts(aisdk_train)\n",
    "Vs_valid = convert_tracks_to_dicts(aisdk_valid)\n",
    "Vs_test = convert_tracks_to_dicts(aisdk_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Normalising data ...\")\n",
    "\n",
    "def normalize(d):\n",
    "    for k in tqdm(list(d.keys())):\n",
    "        v = d[k]\n",
    "        v[:, LAT] = (v[:, LAT] - LAT_MIN) / (LAT_MAX - LAT_MIN)\n",
    "        v[:, LON] = (v[:, LON] - LON_MIN) / (LON_MAX - LON_MIN)\n",
    "        v[:, SOG][v[:, SOG] > SOG_MAX] = SOG_MAX\n",
    "        v[:, SOG] = v[:, SOG] / SOG_MAX\n",
    "        v[:, COG] = v[:, COG] / 360.0\n",
    "    return d \n",
    "\n",
    "Vs_train = normalize(Vs_train)\n",
    "Vs_valid = normalize(Vs_valid)\n",
    "Vs_test = normalize(Vs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename, filedict in zip(\n",
    "    [pkl_filename_train, pkl_filename_valid, pkl_filename_test],\n",
    "    [Vs_train, Vs_valid, Vs_test],\n",
    "):\n",
    "    print(\"Writing to\", os.path.join(out_path, filename))\n",
    "    with open(os.path.join(out_path, filename), \"wb\") as f:\n",
    "        pickle.dump(filedict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "From this point forward, it is recommended to execute the code with the [PY3GPU environment](https://github.com/CIA-Oceanix/GeoTrackNet/blob/master/requirements.yml), as set up by Nguyen et al. (2022).\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AISDK dataset\n",
    "LAT, LON, SOG, COG, NAME, SHIPTYPE, NAV_STT, TIMESTAMP, TRAJ_ID = list(range(9))\n",
    "\n",
    "# Pkl filenames\n",
    "pkl_filename_train = \"aisdk_20180208_train.pkl\"\n",
    "pkl_filename_valid = \"aisdk_20180208_valid.pkl\"\n",
    "pkl_filename_test = \"aisdk_20180208_test.pkl\"\n",
    "\n",
    "# Path to csv files\n",
    "data_path = \"../examples/data/aisdk_20180208_sample/\"\n",
    "csv_filename = \"aisdk_20180208_sample_20000.csv\"\n",
    "\n",
    "# Output path\n",
    "out_path = \"../examples/data/aisdk_20180208_sample/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate AIS mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input pipelines script for Tensorflow graph.\n",
    "This script is adapted from the original script of FIVO.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "dataset_path = os.path.join(data_path, pkl_filename_train)\n",
    "\n",
    "LAT_BINS = 100\n",
    "LON_BINS = 200\n",
    "SOG_BINS = 30\n",
    "COG_BINS = 72\n",
    "\n",
    "\n",
    "def sparse_AIS_to_dense(msgs_, num_timesteps, mmsis):\n",
    "    def create_dense_vect(msg, lat_bins=100, lon_bins=200, sog_bins=30, cog_bins=72):\n",
    "        lat, lon, sog, cog = msg[0], msg[1], msg[2], msg[3]\n",
    "        data_dim = lat_bins + lon_bins + sog_bins + cog_bins\n",
    "        dense_vect = np.zeros(data_dim)\n",
    "        dense_vect[int(lat * lat_bins)] = 1.0\n",
    "        dense_vect[int(lon * lon_bins) + lat_bins] = 1.0\n",
    "        dense_vect[int(sog * sog_bins) + lat_bins + lon_bins] = 1.0\n",
    "        dense_vect[int(cog * cog_bins) + lat_bins + lon_bins + sog_bins] = 1.0\n",
    "        return dense_vect\n",
    "\n",
    "    dense_msgs = []\n",
    "    for msg in msgs_:\n",
    "        dense_msgs.append(\n",
    "            create_dense_vect(\n",
    "                msg,\n",
    "                lat_bins=LAT_BINS,\n",
    "                lon_bins=LON_BINS,\n",
    "                sog_bins=SOG_BINS,\n",
    "                cog_bins=COG_BINS,\n",
    "            )\n",
    "        )\n",
    "    dense_msgs = np.array(dense_msgs)\n",
    "    return dense_msgs, num_timesteps, mmsis\n",
    "\n",
    "\n",
    "dirname = os.path.dirname(dataset_path)\n",
    "\n",
    "try:\n",
    "    with tf.gfile.Open(dataset_path, \"rb\") as f:\n",
    "        Vs = pickle.load(f)\n",
    "except:\n",
    "    with tf.gfile.Open(dataset_path, \"rb\") as f:\n",
    "        Vs = pickle.load(f, encoding=\"latin1\")\n",
    "\n",
    "data_dim = LAT_BINS + LON_BINS + SOG_BINS + COG_BINS\n",
    "\n",
    "mean_all = np.zeros((data_dim,))\n",
    "sum_all = np.zeros((data_dim,))\n",
    "total_ais_msg = 0\n",
    "\n",
    "current_mean = np.zeros((0, data_dim))\n",
    "current_ais_msg = 0\n",
    "\n",
    "count = 0\n",
    "for mmsi in list(Vs.keys()):\n",
    "    count += 1\n",
    "    print(count)\n",
    "    tmp = Vs[mmsi][:, [LAT, LON, SOG, COG]]\n",
    "    tmp[tmp == 1] = 0.99999\n",
    "    current_sparse_matrix, _, _ = sparse_AIS_to_dense(tmp, 0, 0)\n",
    "    #    current_mean = np.mean(current_sparse_matrix,axis = 0)\n",
    "    sum_all += np.sum(current_sparse_matrix, axis=0)\n",
    "    total_ais_msg += len(current_sparse_matrix)\n",
    "\n",
    "mean = sum_all / total_ais_msg\n",
    "\n",
    "print(\"Writing to\", os.path.join(dirname, \"/mean.pkl\"))\n",
    "with open(dirname + \"/mean.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mean, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"train\"\n",
    "dataset_dir = \"../examples/data/aisdk_20180208_sample\"\n",
    "trainingset_name = \"aisdk_20180208_train.pkl\"\n",
    "testset_name = \"aisdk_20180208_test.pkl\"\n",
    "lat_min = 57.4\n",
    "lat_max = 57.9\n",
    "lon_min = 11.3\n",
    "lon_max = 12.1\n",
    "latent_size = 100\n",
    "batch_size = 32\n",
    "num_samples = 16\n",
    "learning_rate = 0.0003\n",
    "%run -i \"../mobiml/models/geotracknet.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN TRAIN\n",
    "# ======================================\n",
    "from mobiml.models.flags_config import config\n",
    "\n",
    "if config.mode == \"train\":\n",
    "    print(config.trainingset_path)\n",
    "    fh = logging.FileHandler(os.path.join(config.logdir, config.log_filename + \".log\"))\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    # get TF logger\n",
    "    logger = logging.getLogger(\"tensorflow\")\n",
    "    logger.addHandler(fh)\n",
    "    runners.run_train(config)\n",
    "\n",
    "else:\n",
    "    with open(config.testset_path, \"rb\") as f:\n",
    "        Vs_test = pickle.load(f)\n",
    "    dataset_size = len(Vs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mobiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
