{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobiML GeoTrackNet Demo\n",
    "\n",
    "Based on: https://github.com/CIA-Oceanix/GeoTrackNet (MIT Licensed, (c) 2018 Duong Nguyen)\n",
    "\n",
    "As presented in Nguyen, D., Vadaine, R., Hajduch, G., Garello, R. (2022). GeoTrackNet - A Maritime Anomaly Detector Using Probabilistic Neural Network Representation of AIS Tracks and A Contrario Detection. In IEEE Transactions on Intelligent Transportation Systems, 23(6).\n",
    "\n",
    "\n",
    "Using data from AISDK: http://web.ais.dk/aisdata/aisdk-2018-02.zip\n",
    "\n",
    "*It is possible to further explore maritime traffic patterns with the TrAISformer (https://github.com/CIA-Oceanix/TrAISformer), which is used for vessel trajectory prediction. The TrAISformer can be trained with AIS data and the preprocessing steps are similar to those of GeoTrackNet. However, the TrAISformer is out of the scope of MobiML and is an optional extension for the user to explore.*\n",
    "\n",
    "## Environments\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "It is recommended to perform the preprocessing steps with the MobiML environment.\n",
    "\n",
    "### Model Training\n",
    "\n",
    "Set up a dedicated GeoTrackNet environment (PY3GPU) to train the model as instructed by Nguyen et al. (2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import movingpandas as mpd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "from mobiml.datasets import AISDK\n",
    "from mobiml.samplers import TemporalSplitter\n",
    "from mobiml.preprocessing import (\n",
    "    TrajectorySplitter,\n",
    "    TrajectoryFilter,\n",
    "    TrajectoryDownsampler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AISDK dataset\n",
    "LAT, LON, SOG, COG, NAME, SHIPTYPE, NAV_STT, TIMESTAMP, TRAJ_ID = list(range(9))\n",
    "\n",
    "EPOCH = datetime(1970, 1, 1)\n",
    "\n",
    "SOG_MIN = 2.0\n",
    "SOG_MAX = 30.0  # SOG is truncated to 30.0 knots max\n",
    "\n",
    "# Pkl filenames\n",
    "pkl_filename_train = \"aisdk_20180208_train.pkl\"\n",
    "pkl_filename_valid = \"aisdk_20180208_valid.pkl\"\n",
    "pkl_filename_test = \"aisdk_20180208_test.pkl\"\n",
    "\n",
    "# Path to csv files\n",
    "data_path = \"data/aisdk_20180208_sample/\"\n",
    "csv_filename = \"aisdk_20180208_sample.csv\"\n",
    "\n",
    "# Output path\n",
    "out_path = \"data/aisdk_20180208_sample/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(data_path, csv_filename)\n",
    "print(f\"{datetime.now()} Loading data from {path}\")\n",
    "aisdk = AISDK(path)  # you can specify a bounding box here to filter the area\n",
    "LON_MIN, LAT_MIN, LON_MAX, LAT_MAX = aisdk.get_bounds()\n",
    "print(\n",
    "    f\"Bounding box:\\nmin_lon: {LON_MIN}\\nmin_lat: {LAT_MIN}\\nmax_lon: {LON_MAX}\\nmax_lat: {LAT_MAX}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk.df = aisdk.df.dropna()\n",
    "aisdk.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After removing missing values we have...\")\n",
    "print(\"Total number of AIS messages: \", aisdk.df.shape[0])\n",
    "print(\"Total number of vessels:\", len(aisdk.df.traj_id.unique()))\n",
    "print(\"Lat min: \", aisdk.df.y.min(), \"Lat max: \", aisdk.df.y.max())\n",
    "print(\"Lon min: \", aisdk.df.x.min(), \"Lon max: \", aisdk.df.x.max())\n",
    "print(\"Time min: \", aisdk.df.timestamp.min(), \"Time max: \", aisdk.df.timestamp.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove 'Moored' and 'At anchor' AIS messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk.df.drop(aisdk.df[(aisdk.df[\"nav_status\"] == \"Moored\") | (aisdk.df[\"nav_status\"] == \"At anchor\")].index, inplace=True)\n",
    "print(\"After removing 'Moored' or 'At anchor' AIS messages we have...\")\n",
    "print(\"Total number of AIS messages: \", aisdk.df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep only 'Cargo', 'Tanker', 'Passenger' vessel types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk.df = aisdk.df[\n",
    "    (aisdk.df[\"ship_type\"] == \"Cargo\")\n",
    "    | (aisdk.df[\"ship_type\"] == \"Tanker\")\n",
    "    | (aisdk.df[\"ship_type\"] == \"Passenger\")\n",
    "]\n",
    "print(\"After keeping only 'Cargo', 'Tanker' or 'Passenger' AIS messages we have...\")\n",
    "print(\"Total number of AIS messages: \", aisdk.df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split trajectories with observation gaps > 2 hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk = TrajectorySplitter(aisdk).split(observation_gap=timedelta(hours=2))\n",
    "print(\"After splitting trajectories with observation gaps we have...\")\n",
    "print(\"Total number of AIS messages: \", aisdk.df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop trajectories with fewer than $Points_{min}$ locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk = TrajectoryFilter(aisdk).filter_min_pts(min_pts=20)\n",
    "print(\"After removing trajectories with too few points we have...\")\n",
    "print(\"Total number of AIS messages: \", aisdk.df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop speed outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk = TrajectoryFilter(aisdk).filter_speed(min_speed=SOG_MIN, max_speed=SOG_MAX)\n",
    "print(\"After removing speed outliers by setting a minimum and maximum speed we have...\")\n",
    "print(\"Total number of AIS messages: \", aisdk.df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = aisdk.to_trajs() #  mpd.TrajectoryCollection(aisdk.df, \"traj_id\", t=\"timestamp\", x=\"x\", y=\"y\")\n",
    "traj_gdf = tc.to_traj_gdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also want to remove trajectories based on their overall average speed rather than the SOG values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in traj_gdf.iterrows():\n",
    "    traj_gdf.loc[index, \"speed_ok\"] = (\n",
    "        tc.trajectories[index].get_length()\n",
    "        / tc.trajectories[index].get_duration().total_seconds()\n",
    "        > 1.02889  # 2 knots\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_gdf = traj_gdf[traj_gdf[\"speed_ok\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk.df = pd.merge(aisdk.df, traj_gdf[\"traj_id\"], how=\"inner\")\n",
    "print(\"After removing speed outliers based on length and duration we have...\")\n",
    "print(\"Total number of AIS messages: \", aisdk.df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data preparation\n",
    "#### Subsample AIS tracks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk = TrajectoryDownsampler(aisdk).subsample(min_dt_sec=60)\n",
    "print(\"After subsampling AIS tracks we have...\")\n",
    "print(\"Total number of AIS messages: \", aisdk.df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal train/valid/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk = TemporalSplitter(aisdk).split_hr()\n",
    "aisdk.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk_train = aisdk.df[(aisdk.df[\"split\"] == 1.0)]\n",
    "aisdk_valid = aisdk.df[(aisdk.df[\"split\"] == 2.0)]\n",
    "aisdk_test = aisdk.df[(aisdk.df[\"split\"] == 3.0)]\n",
    "\n",
    "print(\"Total number of AIS messages: \", len(aisdk.df))\n",
    "print(\"Number of msgs in the training set: \", len(aisdk_train))\n",
    "print(\"Number of msgs in the validation set: \", len(aisdk_valid))\n",
    "print(\"Number of msgs in the test set: \", len(aisdk_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column_order=[\"y\", \"x\", \"speed\", \"direction\", \"Name\", \"ship_type\", \"nav_status\", \"timestamp\", \"traj_id\"]\n",
    "aisdk_train = aisdk_train[target_column_order].reset_index(drop=True)\n",
    "aisdk_valid = aisdk_valid[target_column_order].reset_index(drop=True)\n",
    "aisdk_test = aisdk_test[target_column_order].reset_index(drop=True)\n",
    "aisdk_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk_train[\"timestamp\"] = (aisdk_train[\"timestamp\"].astype(int) / 1_000_000_000).astype(int)\n",
    "aisdk_valid[\"timestamp\"] = (aisdk_valid[\"timestamp\"].astype(int) / 1_000_000_000).astype(int)\n",
    "aisdk_test[\"timestamp\"]  = (aisdk_test[\"timestamp\"].astype(int) / 1_000_000_000).astype(int)\n",
    "aisdk_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format to ndarrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk_train = np.array(aisdk_train)\n",
    "aisdk_valid = np.array(aisdk_valid)\n",
    "aisdk_test = np.array(aisdk_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging into dict\n",
    "Creating AIS tracks from the list of AIS messages. Each AIS track is formatted by a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Convert to dicts of vessel's tracks...\")\n",
    "\n",
    "def convert_tracks_to_dicts(tracks):\n",
    "    d = dict()\n",
    "    for v_msg in tqdm(tracks):\n",
    "        mmsi = int(v_msg[TRAJ_ID])\n",
    "        if not (mmsi in list(d.keys())):\n",
    "            d[mmsi] = np.empty((0, 9))\n",
    "        d[mmsi] = np.concatenate(\n",
    "            (d[mmsi], np.expand_dims(v_msg[:9], 0)), axis=0\n",
    "        )\n",
    "    for key in tqdm(list(d.keys())):\n",
    "        d[key] = np.array(\n",
    "            sorted(d[key], key=lambda m_entry: m_entry[TIMESTAMP])\n",
    "        )\n",
    "    return d\n",
    "\n",
    "Vs_train = convert_tracks_to_dicts(aisdk_train)\n",
    "Vs_valid = convert_tracks_to_dicts(aisdk_valid)\n",
    "Vs_test = convert_tracks_to_dicts(aisdk_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Normalising data ...\")\n",
    "\n",
    "def normalize(d):\n",
    "    for k in tqdm(list(d.keys())):\n",
    "        v = d[k]\n",
    "        v[:, LAT] = (v[:, LAT] - LAT_MIN) / (LAT_MAX - LAT_MIN)\n",
    "        v[:, LON] = (v[:, LON] - LON_MIN) / (LON_MAX - LON_MIN)\n",
    "        v[:, SOG][v[:, SOG] > SOG_MAX] = SOG_MAX\n",
    "        v[:, SOG] = v[:, SOG] / SOG_MAX\n",
    "        v[:, COG] = v[:, COG] / 360.0\n",
    "    return d \n",
    "\n",
    "Vs_train = normalize(Vs_train)\n",
    "Vs_valid = normalize(Vs_valid)\n",
    "Vs_test = normalize(Vs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename, filedict in zip(\n",
    "    [pkl_filename_train, pkl_filename_valid, pkl_filename_test],\n",
    "    [Vs_train, Vs_valid, Vs_test],\n",
    "):\n",
    "    print(\"Writing to\", os.path.join(out_path, filename))\n",
    "    with open(os.path.join(out_path, filename), \"wb\") as f:\n",
    "        pickle.dump(filedict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "From this point forward, it is recommended to execute the code with the [PY3GPU environment](https://github.com/CIA-Oceanix/GeoTrackNet/blob/master/requirements.yml), as set up by Nguyen et al. (2022).\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# AISDK dataset\n",
    "LAT, LON, SOG, COG, NAME, SHIPTYPE, NAV_STT, TIMESTAMP, TRAJ_ID = list(range(9))\n",
    "\n",
    "# Pkl filenames\n",
    "pkl_filename_train = \"aisdk_20180208_train.pkl\"\n",
    "pkl_filename_valid = \"aisdk_20180208_valid.pkl\"\n",
    "pkl_filename_test = \"aisdk_20180208_test.pkl\"\n",
    "\n",
    "data_path = \"../examples/data/aisdk_20180208_sample/\"\n",
    "dataset_path = os.path.join(data_path, pkl_filename_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate AIS mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "LAT_BINS = 100\n",
    "LON_BINS = 200\n",
    "SOG_BINS = 30\n",
    "COG_BINS = 72\n",
    "\n",
    "\n",
    "def sparse_AIS_to_dense(msgs_, num_timesteps, mmsis):\n",
    "    def create_dense_vect(msg, lat_bins=100, lon_bins=200, sog_bins=30, cog_bins=72):\n",
    "        lat, lon, sog, cog = msg[0], msg[1], msg[2], msg[3]\n",
    "        data_dim = lat_bins + lon_bins + sog_bins + cog_bins\n",
    "        dense_vect = np.zeros(data_dim)\n",
    "        dense_vect[int(lat * lat_bins)] = 1.0\n",
    "        dense_vect[int(lon * lon_bins) + lat_bins] = 1.0\n",
    "        dense_vect[int(sog * sog_bins) + lat_bins + lon_bins] = 1.0\n",
    "        dense_vect[int(cog * cog_bins) + lat_bins + lon_bins + sog_bins] = 1.0\n",
    "        return dense_vect\n",
    "\n",
    "    dense_msgs = []\n",
    "    for msg in msgs_:\n",
    "        dense_msgs.append(\n",
    "            create_dense_vect(\n",
    "                msg,\n",
    "                lat_bins=LAT_BINS,\n",
    "                lon_bins=LON_BINS,\n",
    "                sog_bins=SOG_BINS,\n",
    "                cog_bins=COG_BINS,\n",
    "            )\n",
    "        )\n",
    "    dense_msgs = np.array(dense_msgs)\n",
    "    return dense_msgs, num_timesteps, mmsis\n",
    "\n",
    "\n",
    "dirname = os.path.dirname(dataset_path)\n",
    "\n",
    "try:\n",
    "    with tf.gfile.Open(dataset_path, \"rb\") as f:\n",
    "        Vs = pickle.load(f)\n",
    "except:\n",
    "    with tf.gfile.Open(dataset_path, \"rb\") as f:\n",
    "        Vs = pickle.load(f, encoding=\"latin1\")\n",
    "\n",
    "data_dim = LAT_BINS + LON_BINS + SOG_BINS + COG_BINS\n",
    "\n",
    "mean_all = np.zeros((data_dim,))\n",
    "sum_all = np.zeros((data_dim,))\n",
    "total_ais_msg = 0\n",
    "\n",
    "current_mean = np.zeros((0, data_dim))\n",
    "current_ais_msg = 0\n",
    "\n",
    "count = 0\n",
    "for mmsi in list(Vs.keys()):\n",
    "    count += 1\n",
    "    print(count)\n",
    "    tmp = Vs[mmsi][:, [LAT, LON, SOG, COG]]\n",
    "    tmp[tmp == 1] = 0.99999\n",
    "    current_sparse_matrix, _, _ = sparse_AIS_to_dense(tmp, 0, 0)\n",
    "    #    current_mean = np.mean(current_sparse_matrix,axis = 0)\n",
    "    sum_all += np.sum(current_sparse_matrix, axis=0)\n",
    "    total_ais_msg += len(current_sparse_matrix)\n",
    "\n",
    "mean = sum_all / total_ais_msg\n",
    "\n",
    "print(\"Writing to\", os.path.join(dirname, \"/mean.pkl\"))\n",
    "with open(dirname + \"/mean.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mean, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "#### Step 1: Training the Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import mobiml.models.geotracknet.runners as runners\n",
    "from mobiml.models.geotracknet.flags_config import config\n",
    "\n",
    "print(config.trainingset_path)\n",
    "fh = logging.FileHandler(os.path.join(config.logdir, config.log_filename + \".log\"))\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "# get TF logger\n",
    "logger = logging.getLogger(\"tensorflow\")\n",
    "logger.addHandler(fh)\n",
    "runners.run_train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Running task-specific submodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "\n",
    "with open(config.testset_path, \"rb\") as f:\n",
    "    Vs_test = pickle.load(f)\n",
    "dataset_size = len(Vs_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = None\n",
    "\n",
    "tf.Graph().as_default()\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "inputs, targets, mmsis, time_starts, time_ends, lengths, model = (\n",
    "    runners.create_dataset_and_model(config, shuffle=False, repeat=False)\n",
    ")\n",
    "\n",
    "if config.mode == \"traj_reconstruction\":\n",
    "    config.missing_data = True\n",
    "\n",
    "track_sample, track_true, log_weights, ll_per_t, ll_acc, _, _, _ = (\n",
    "    runners.create_eval_graph(inputs, targets, lengths, model, config)\n",
    ")\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.train.SingularMonitoredSession()\n",
    "runners.wait_for_checkpoint(saver, sess, config.logdir)\n",
    "step = sess.run(global_step)\n",
    "\n",
    "if step is None:\n",
    "    # The log filename contains the step.\n",
    "    index_filename = sorted(glob.glob(config.logdir+\"/*.index\"))[-1] # the lastest step\n",
    "    step = int(index_filename.split(\".index\")[0].split(\"ckpt-\")[-1])\n",
    "    \n",
    "\n",
    "print(\"Global step: \", step)\n",
    "outputs_path = \"results/\"\\\n",
    "            + config.trainingset_path.split(\"/\")[-2] + \"/\"\\\n",
    "            + \"logprob-\"\\\n",
    "            + os.path.basename(config.trainingset_name) + \"-\"\\\n",
    "            + os.path.basename(config.testset_name) + \"-\"\\\n",
    "            + str(config.latent_size)\\\n",
    "            + \"-missing_data-\" + str(config.missing_data)\\\n",
    "            + \"-step-\"+str(step)\\\n",
    "            + \".pkl\"\n",
    "if not os.path.exists(os.path.dirname(outputs_path)):\n",
    "    os.makedirs(os.path.dirname(outputs_path))\n",
    "\n",
    "save_dir = \"results/\"\\\n",
    "            + config.trainingset_path.split(\"/\")[-2] + \"/\"\\\n",
    "            + \"local_logprob-\"\\\n",
    "            + os.path.basename(config.trainingset_name) + \"-\"\\\n",
    "            + os.path.basename(config.testset_name).replace(\"test\",\"valid\") + \"-\"\\\n",
    "            + str(config.latent_size) + \"-\"\\\n",
    "            + \"missing_data-\" + str(config.missing_data)\\\n",
    "            + \"-step-\"+str(step)\\\n",
    "            +\"/\"     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### save_logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "FIG_DPI = 150\n",
    "\n",
    "l_dict = []\n",
    "for d_i in tqdm(list(range(math.ceil(dataset_size / config.batch_size)))):\n",
    "    inp, tar, mmsi, t_start, t_end, seq_len, log_weights_np, true_np, ll_t = (\n",
    "        sess.run(\n",
    "            [\n",
    "                inputs,\n",
    "                targets,\n",
    "                mmsis,\n",
    "                time_starts,\n",
    "                time_ends,\n",
    "                lengths,\n",
    "                log_weights,\n",
    "                track_true,\n",
    "                ll_per_t,\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    for d_idx_inbatch in range(inp.shape[1]):\n",
    "        D = dict()\n",
    "        seq_len_d = seq_len[d_idx_inbatch]\n",
    "        D[\"seq\"] = np.nonzero(tar[:seq_len_d, d_idx_inbatch, :])[1].reshape(-1, 4)\n",
    "        D[\"t_start\"] = t_start[d_idx_inbatch]\n",
    "        D[\"t_end\"] = t_end[d_idx_inbatch]\n",
    "        D[\"mmsi\"] = mmsi[d_idx_inbatch]\n",
    "        D[\"log_weights\"] = log_weights_np[:seq_len_d, :, d_idx_inbatch]\n",
    "        l_dict.append(D)\n",
    "with open(outputs_path, \"wb\") as f:\n",
    "    pickle.dump(l_dict, f)\n",
    "\n",
    "v_logprob = np.empty((0,))\n",
    "v_logprob_stable = np.empty((0,))\n",
    "\n",
    "count = 0\n",
    "for D in tqdm(l_dict):\n",
    "    log_weights_np = D[\"log_weights\"]\n",
    "    ll_t = np.mean(log_weights_np)\n",
    "    v_logprob = np.concatenate((v_logprob, [ll_t]))\n",
    "\n",
    "d_mean = np.mean(v_logprob)\n",
    "d_std = np.std(v_logprob)\n",
    "d_thresh = d_mean - 3 * d_std\n",
    "\n",
    "plt.figure(figsize=(1920/FIG_DPI, 640/FIG_DPI), dpi=FIG_DPI)\n",
    "plt.plot(v_logprob,'o')\n",
    "plt.title(\"Log likelihood \" + os.path.basename(config.testset_name)\\\n",
    "            + \", mean = {0:02f}, std = {1:02f}, threshold = {2:02f}\".format(d_mean, d_std, d_thresh))\n",
    "plt.plot([0,len(v_logprob)], [d_thresh, d_thresh],'r')\n",
    "\n",
    "plt.xlim([0,len(v_logprob)])\n",
    "fig_name = \"results/\"\\\n",
    "        + config.trainingset_path.split(\"/\")[-2] + \"/\" \\\n",
    "        + \"logprob-\" \\\n",
    "        + config.bound + \"-\"\\\n",
    "        + os.path.basename(config.trainingset_name) + \"-\"\\\n",
    "        + os.path.basename(config.testset_name)\\\n",
    "        + \"-latent_size-\" + str(config.latent_size)\\\n",
    "        + \"-ll_thresh\" + str(round(d_thresh, 2))\\\n",
    "        + \"-missing_data-\" + str(config.missing_data)\\\n",
    "        + \"-step-\"+str(step)\\\n",
    "        + \".png\"\n",
    "plt.savefig(fig_name,dpi = FIG_DPI)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](fig_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=fig_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### local_logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mobiml.models.geotracknet.utils as utils\n",
    "\n",
    "LOGPROB_MEAN_MIN = -10.0\n",
    "LOGPROB_STD_MAX = 5\n",
    "\n",
    "LAT_RANGE = config.lat_max - config.lat_min\n",
    "LON_RANGE = config.lon_max - config.lon_min\n",
    "FIG_W = 960\n",
    "FIG_H = int(FIG_W*LAT_RANGE/LON_RANGE)\n",
    "\n",
    "m_map_logprob_std = np.zeros(shape=(config.n_lat_cells,config.n_lon_cells))\n",
    "m_map_logprob_mean = np.zeros(shape=(config.n_lat_cells,config.n_lon_cells))\n",
    "m_map_density = np.zeros(shape=(config.n_lat_cells,config.n_lon_cells))\n",
    "v_logprob = np.empty((0,))\n",
    "v_mmsi = np.empty((0,))\n",
    "Map_logprob = dict()\n",
    "for row  in range(config.n_lat_cells):\n",
    "    for col in range(config.n_lon_cells):\n",
    "        Map_logprob[ str(str(row)+\",\"+str(col))] = []\n",
    "\n",
    "# Load logprob\n",
    "with open(outputs_path,\"rb\") as f:\n",
    "    l_dict = pickle.load(f)\n",
    "\n",
    "print(\"Calculating the logprob map...\")\n",
    "for D in tqdm(l_dict):\n",
    "    tmp = D[\"seq\"]\n",
    "    log_weights_np = D[\"log_weights\"]\n",
    "    for d_timestep in range(2*6,len(tmp)):\n",
    "        try:\n",
    "            row = int(tmp[d_timestep,0]*0.01/config.cell_lat_reso)\n",
    "            col = int((tmp[d_timestep,1]-config.onehot_lat_bins)*0.01/config.cell_lat_reso)\n",
    "            Map_logprob[str(row)+\",\"+str(col)].append(np.mean(log_weights_np[d_timestep,:]))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# Remove outliers\n",
    "for row  in range(config.n_lat_cells):\n",
    "    for col in range(config.n_lon_cells):\n",
    "        s_key = str(row)+\",\"+str(col) \n",
    "        Map_logprob[s_key] = utils.remove_gaussian_outlier(np.array(Map_logprob[s_key]))\n",
    "        m_map_logprob_mean[row,col] = np.mean(Map_logprob[s_key])\n",
    "        m_map_logprob_std[row,col] = np.std(Map_logprob[s_key])\n",
    "        m_map_density[row,col] = len(Map_logprob[s_key])\n",
    "\n",
    "# Save to disk\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "np.save(save_dir+\"map_density-\"+str(config.cell_lat_reso)+\"-\"+str(config.cell_lon_reso),m_map_density)\n",
    "with open(os.path.join(save_dir,\"Map_logprob-\"+str(config.cell_lat_reso)+\"-\"+str(config.cell_lon_reso)+\".pkl\"),\"wb\") as f:\n",
    "    pickle.dump(Map_logprob,f)\n",
    "\n",
    "# Show the map\n",
    "utils.show_logprob_map(m_map_logprob_mean, m_map_logprob_std, save_dir, \n",
    "                        logprob_mean_min = LOGPROB_MEAN_MIN,\n",
    "                        logprob_std_max = LOGPROB_STD_MAX,\n",
    "                        fig_w = FIG_W, fig_h = FIG_H,\n",
    "                        )    \n",
    "\n",
    "print(f'Maps stored saved to: {os.path.join(save_dir, \"logprob_std_map.png\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=os.path.join(save_dir, \"logprob_std_map.png\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### contrario_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "import mobiml.models.geotracknet.contrario_utils as contrario_utils\n",
    "\n",
    "with open(os.path.join(save_dir,\"Map_logprob-\"+\\\n",
    "            str(config.cell_lat_reso)+\"-\"+str(config.cell_lat_reso)+\".pkl\"),\"rb\") as f:\n",
    "    Map_logprob = pickle.load(f)\n",
    "# Load the logprob\n",
    "with open(outputs_path,\"rb\") as f:\n",
    "    l_dict = pickle.load(f)\n",
    "d_i = 0\n",
    "v_mean_log = []\n",
    "l_v_A = []\n",
    "v_buffer_count = []\n",
    "length_track = len(l_dict[0][\"seq\"])\n",
    "l_dict_anomaly = []\n",
    "n_error = 0\n",
    "for D in tqdm(l_dict):\n",
    "    try:\n",
    "    # if True:\n",
    "        tmp = D[\"seq\"]\n",
    "        m_log_weights_np = D[\"log_weights\"]\n",
    "        v_A = np.zeros(len(tmp))\n",
    "        for d_timestep in range(2*6,len(tmp)):\n",
    "            d_row = int(tmp[d_timestep,0]*config.onehot_lat_reso/config.cell_lat_reso)\n",
    "            d_col = int((tmp[d_timestep,1]-config.onehot_lat_bins)*config.onehot_lat_reso/config.cell_lon_reso)\n",
    "            d_logprob_t = np.mean(m_log_weights_np[d_timestep,:])\n",
    "\n",
    "            # KDE\n",
    "            l_local_log_prod = Map_logprob[str(d_row)+\",\"+str(d_col)]\n",
    "            if len(l_local_log_prod) < 2:\n",
    "                v_A[d_timestep] = 2\n",
    "            else:\n",
    "                kernel = stats.gaussian_kde(l_local_log_prod)\n",
    "                cdf = kernel.integrate_box_1d(-np.inf,d_logprob_t)\n",
    "                if cdf < 0.1:\n",
    "                    v_A[d_timestep] = 1\n",
    "        v_A = v_A[12:]\n",
    "        v_anomalies = np.zeros(len(v_A))\n",
    "        for d_i_4h in range(0,len(v_A)+1-24):\n",
    "            v_A_4h = v_A[d_i_4h:d_i_4h+24]\n",
    "            v_anomalies_i = contrario_utils.contrario_detection(v_A_4h,config.contrario_eps)\n",
    "            v_anomalies[d_i_4h:d_i_4h+24][v_anomalies_i==1] = 1\n",
    "\n",
    "        if len(contrario_utils.nonzero_segments(v_anomalies)) > 0:\n",
    "            D[\"anomaly_idx\"] = v_anomalies\n",
    "            l_dict_anomaly.append(D)\n",
    "    except:\n",
    "        n_error += 1\n",
    "print(\"Number of processed tracks: \",len(l_dict))\n",
    "print(\"Number of abnormal tracks: \",len(l_dict_anomaly)) \n",
    "print(\"Number of errors: \",n_error)\n",
    "\n",
    "# Save to disk\n",
    "n_anomalies = len(l_dict_anomaly)\n",
    "save_filename = os.path.basename(config.trainingset_name)\\\n",
    "                +\"-\" + os.path.basename(config.trainingset_name)\\\n",
    "                +\"-\" + str(config.latent_size)\\\n",
    "                +\"-missing_data-\"+str(config.missing_data)\\\n",
    "                +\"-step-\"+str(step)\\\n",
    "                +\".pkl\"\n",
    "save_pkl_filename = os.path.join(save_dir,\"List_abnormal_tracks-\"+save_filename)\n",
    "with open(save_pkl_filename,\"wb\") as f:\n",
    "    pickle.dump(l_dict_anomaly,f)\n",
    "\n",
    "## Plot\n",
    "with open(config.trainingset_path,\"rb\") as f:\n",
    "    Vs_train = pickle.load(f)\n",
    "with open(config.testset_path,\"rb\") as f:\n",
    "    Vs_test = pickle.load(f)\n",
    "\n",
    "save_filename = \"Abnormal_tracks\"\\\n",
    "            + \"-\" + os.path.basename(config.trainingset_name)\\\n",
    "            + \"-\" + os.path.basename(config.testset_name)\\\n",
    "            + \"-latent_size-\" + str(config.latent_size)\\\n",
    "            + \"-step-\"+str(step)\\\n",
    "            + \"-eps-\"+str(config.contrario_eps)\\\n",
    "            + \"-\" + str(n_anomalies)\\\n",
    "            + \".png\"\n",
    "\n",
    "# Plot abnormal tracks with the tracks in the training set as the background\n",
    "utils.plot_abnormal_tracks(Vs_train,l_dict_anomaly,\n",
    "                    os.path.join(save_dir,save_filename),\n",
    "                    config.lat_min,config.lat_max,config.lon_min,config.lon_max,\n",
    "                    config.onehot_lat_bins,config.onehot_lon_bins,\n",
    "                    background_cmap = \"Blues\",\n",
    "                    fig_w = FIG_W, fig_h = FIG_H,\n",
    "                )\n",
    "plt.close()\n",
    "# Plot abnormal tracks with the tracks in the test set as the background\n",
    "utils.plot_abnormal_tracks(Vs_test,l_dict_anomaly,\n",
    "                    os.path.join(save_dir,save_filename.replace(\"Abnormal_tracks\",\"Abnormal_tracks2\")),\n",
    "                    config.lat_min,config.lat_max,config.lon_min,config.lon_max,\n",
    "                    config.onehot_lat_bins,config.onehot_lon_bins,\n",
    "                    background_cmap = \"Greens\",\n",
    "                    fig_w = FIG_W, fig_h = FIG_H,\n",
    "                )\n",
    "plt.close()   \n",
    "# Save abnormal tracks to csv file\n",
    "with open(os.path.join(save_dir,save_filename.replace(\".png\",\".csv\")),\"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"MMSI\",\"Time_start\",\"Time_end\",\"Timestamp_start\",\"Timestamp_end\"])\n",
    "    for D in l_dict_anomaly:\n",
    "        writer.writerow([D[\"mmsi\"],\n",
    "            datetime.utcfromtimestamp(D[\"t_start\"]).strftime('%Y-%m-%d %H:%M:%SZ'),\n",
    "            datetime.utcfromtimestamp(D[\"t_end\"]).strftime('%Y-%m-%d %H:%M:%SZ'),\n",
    "            D[\"t_start\"],D[\"t_end\"]])\n",
    "        \n",
    "print(f'Maps stored saved to: {save_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=os.path.join(save_dir,save_filename.replace(\"Abnormal_tracks\",\"Abnormal_tracks2\"))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mobiml-viz",
   "language": "python",
   "name": "mobiml-viz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
