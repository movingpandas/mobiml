{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# coding: utf-8\n",
    "\n",
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2018 Duong Nguyen\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "# ==============================================================================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobiML GeoTrackNet Demo\n",
    "\n",
    "Based on: https://github.com/CIA-Oceanix/GeoTrackNet\n",
    "\n",
    "As presented in Nguyen, D., Vadaine, R., Hajduch, G., Garello, R. (2022). GeoTrackNet - A Maritime Anomaly Detector Using Probabilistic Neural Network Representation of AIS Tracks and A Contrario Detection. In IEEE Transactions on Intelligent Transportation Systems, 23(6).\n",
    "\n",
    "Using data from AISDK: http://web.ais.dk/aisdata/aisdk-2018-02.zip\n",
    "\n",
    "It is recommended to perform the preprocessing steps with the MobiML environment. Set up a dedicated GeoTrackNet environment (PY3GPU) to train the model as instructed by Nguyen et al. (2022).\n",
    "\n",
    "It is possible to further explore maritime traffic patterns with the TrAISformer (https://github.com/CIA-Oceanix/TrAISformer), which is used for vessel trajectory prediction. The TrAISformer can be trained with AIS data and the preprocessing steps are similar to those of GeoTrackNet. However, the TrAISformer is out of the scope of MobiML and is an optional extension for the user to explore. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import movingpandas as mpd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from mobiml.datasets import AISDK\n",
    "from mobiml.preprocessing import (\n",
    "    TrajectorySplitter,\n",
    "    TrajectoryFilter,\n",
    "    TrajectorySubsampler,\n",
    ")\n",
    "from mobiml.transforms import TemporalSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AISDK dataset\n",
    "LAT, LON, SOG, COG, NAME, SHIPTYPE, NAV_STT, TIMESTAMP, TRAJ_ID = list(range(9))\n",
    "\n",
    "EPOCH = datetime(1970, 1, 1)\n",
    "\n",
    "SOG_MIN = 2.0\n",
    "SOG_MAX = 30.0  # SOG is truncated to 30.0 knots max\n",
    "\n",
    "# Pkl filenames\n",
    "pkl_filename_train = \"aisdk_20180208_train.pkl\"\n",
    "pkl_filename_valid = \"aisdk_20180208_valid.pkl\"\n",
    "pkl_filename_test = \"aisdk_20180208_test.pkl\"\n",
    "\n",
    "# Path to csv files\n",
    "data_path = \"../examples/data/aisdk_20180208_sample/\"\n",
    "csv_filename = \"aisdk_20180208_sample.csv\"\n",
    "\n",
    "# Output path\n",
    "out_path = \"../examples/data/aisdk_20180208_sample/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set up bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(data_path, csv_filename))\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    data, geometry=gpd.points_from_xy(data.Longitude, data.Latitude), crs=\"EPSG:4326\"\n",
    ")\n",
    "min_lon, min_lat, max_lon, max_lat = gdf.total_bounds\n",
    "\n",
    "LAT_MIN = min_lat\n",
    "LAT_MAX = max_lat\n",
    "LON_MIN = min_lon\n",
    "LON_MAX = max_lon\n",
    "\n",
    "print(\n",
    "    f\"Bounding box set to:\\nmin_lon: {min_lon}\\nmin_lat: {min_lat}\\nmax_lon: {max_lon}\\nmax_lat: {max_lat}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter to ROI by specifying bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(data_path, csv_filename)\n",
    "print(f\"{datetime.now()} Loading data from {path}\")\n",
    "aisdk = AISDK(path, min_lon, min_lat, max_lon, max_lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk.df = aisdk.df.dropna()\n",
    "aisdk.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After removing missing values we have...\")\n",
    "print(\"Total number of AIS messages: \", aisdk.df.shape[0])\n",
    "print(\"Total number of vessels:\", len(aisdk.df.traj_id.unique()))\n",
    "print(\"Lat min: \", aisdk.df.y.min(), \"Lat max: \", aisdk.df.y.max())\n",
    "print(\"Lon min: \", aisdk.df.x.min(), \"Lon max: \", aisdk.df.x.max())\n",
    "print(\"Time min: \", aisdk.df.timestamp.min(), \"Time max: \", aisdk.df.timestamp.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove 'Moored' and 'At anchor' AIS messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moored_at_anchor = aisdk.df[\n",
    "    (aisdk.df[\"nav_status\"] == \"Moored\") | (aisdk.df[\"nav_status\"] == \"At anchor\")\n",
    "]\n",
    "aisdk.df = pd.concat([aisdk.df, moored_at_anchor]).drop_duplicates(keep=False)\n",
    "print(\"After removing 'Moored' or 'At anchor' AIS messages we have...\")\n",
    "print(\"Total number of AIS messages: \", aisdk.df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keep only 'Cargo', 'Tanker', 'Passenger' vessel types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk.df = aisdk.df[\n",
    "    (aisdk.df[\"ship_type\"] == \"Cargo\")\n",
    "    | (aisdk.df[\"ship_type\"] == \"Tanker\")\n",
    "    | (aisdk.df[\"ship_type\"] == \"Passenger\")\n",
    "]\n",
    "print(\"After keeping only 'Cargo', 'Tanker' or 'Passenger' AIS messages we have...\")\n",
    "print(\"Total number of AIS messages: \", aisdk.df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split trajectories with observation gaps > 2 hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk = TrajectorySplitter(aisdk).split(observation_gap=timedelta(hours=2))\n",
    "print(\"After splitting trajectories with observation gaps we have...\")\n",
    "print(\"Total number of AIS messages: \", aisdk.df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop trajectories with fewer than $Points_{min}$ locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk = TrajectoryFilter(aisdk).filter_min_pts(min_pts=20)\n",
    "print(\"After removing trajectories with too few points we have...\")\n",
    "print(\"Total number of AIS messages: \", aisdk.df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop speed outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk = TrajectoryFilter(aisdk).filter_speed(min_speed=SOG_MIN, max_speed=SOG_MAX)\n",
    "print(\"After removing speed outliers by setting a minimum and maximum speed we have...\")\n",
    "print(\"Total number of AIS messages: \", aisdk.df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = mpd.TrajectoryCollection(aisdk.df, \"traj_id\", t=\"timestamp\", x=\"x\", y=\"y\")\n",
    "traj_gdf = tc.to_traj_gdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in traj_gdf.iterrows():\n",
    "    traj_gdf.loc[index, \"x\"] = (\n",
    "        tc.trajectories[index].get_length()\n",
    "        / tc.trajectories[index].get_duration().total_seconds()\n",
    "        > 1.02889\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_gdf = traj_gdf[traj_gdf[\"x\"] == True]\n",
    "traj_gdf = traj_gdf[\"traj_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk.df = pd.merge(aisdk.df, traj_gdf, how=\"inner\")\n",
    "print(\"After removing speed outliers based on length and duration we have...\")\n",
    "print(\"Total number of AIS messages: \", aisdk.df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Subsample AIS tracks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk = TrajectorySubsampler(aisdk).subsample(min_dt_sec=60)\n",
    "print(\"After subsampling AIS tracks we have...\")\n",
    "print(\"Total number of AIS messages: \", aisdk.df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Temporal train/valid/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk = TemporalSplitter(aisdk).split_hr()\n",
    "aisdk.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk_train = aisdk.df[(aisdk.df[\"split\"] == 1.0)]\n",
    "aisdk_valid = aisdk.df[(aisdk.df[\"split\"] == 2.0)]\n",
    "aisdk_test = aisdk.df[(aisdk.df[\"split\"] == 3.0)]\n",
    "\n",
    "print(\"Total number of AIS messages: \", len(aisdk.df))\n",
    "print(\"Number of msgs in the training set: \", len(aisdk_train))\n",
    "print(\"Number of msgs in the validation set: \", len(aisdk_valid))\n",
    "print(\"Number of msgs in the test set: \", len(aisdk_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk_train = aisdk_train[\n",
    "    [\n",
    "        \"y\",\n",
    "        \"x\",\n",
    "        \"speed\",\n",
    "        \"direction\",\n",
    "        \"Name\",\n",
    "        \"ship_type\",\n",
    "        \"nav_status\",\n",
    "        \"timestamp\",\n",
    "        \"traj_id\",\n",
    "    ]\n",
    "].reset_index()\n",
    "aisdk_valid = aisdk_valid[\n",
    "    [\n",
    "        \"y\",\n",
    "        \"x\",\n",
    "        \"speed\",\n",
    "        \"direction\",\n",
    "        \"Name\",\n",
    "        \"ship_type\",\n",
    "        \"nav_status\",\n",
    "        \"timestamp\",\n",
    "        \"traj_id\",\n",
    "    ]\n",
    "].reset_index()\n",
    "aisdk_test = aisdk_test[\n",
    "    [\n",
    "        \"y\",\n",
    "        \"x\",\n",
    "        \"speed\",\n",
    "        \"direction\",\n",
    "        \"Name\",\n",
    "        \"ship_type\",\n",
    "        \"nav_status\",\n",
    "        \"timestamp\",\n",
    "        \"traj_id\",\n",
    "    ]\n",
    "].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk_train = aisdk_train.drop(columns=[\"index\"])\n",
    "aisdk_valid = aisdk_valid.drop(columns=[\"index\"])\n",
    "aisdk_test = aisdk_test.drop(columns=[\"index\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Format timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.to_datetime(aisdk_train[\"timestamp\"], format=\"%d/%m/%Y %H:%M:%S\")\n",
    "aisdk_train[\"timestamp\"] = (dates - pd.Timestamp(EPOCH)) // pd.Timedelta(\"1s\")\n",
    "\n",
    "dates = pd.to_datetime(aisdk_valid[\"timestamp\"], format=\"%d/%m/%Y %H:%M:%S\")\n",
    "aisdk_valid[\"timestamp\"] = (dates - pd.Timestamp(EPOCH)) // pd.Timedelta(\"1s\")\n",
    "\n",
    "dates = pd.to_datetime(aisdk_test[\"timestamp\"], format=\"%d/%m/%Y %H:%M:%S\")\n",
    "aisdk_test[\"timestamp\"] = (dates - pd.Timestamp(EPOCH)) // pd.Timedelta(\"1s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Format to ndarrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisdk_train = np.array(aisdk_train)\n",
    "aisdk_valid = np.array(aisdk_valid)\n",
    "aisdk_test = np.array(aisdk_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merging into dict\n",
    "Creating AIS tracks from the list of AIS messages. Each AIS track is formatted by a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Convert to dicts of vessel's tracks...\")\n",
    "\n",
    "# Training set\n",
    "Vs_train = dict()\n",
    "for v_msg in tqdm(aisdk_train):\n",
    "    mmsi = int(v_msg[TRAJ_ID])\n",
    "    if not (mmsi in list(Vs_train.keys())):\n",
    "        Vs_train[mmsi] = np.empty((0, 9))\n",
    "    Vs_train[mmsi] = np.concatenate(\n",
    "        (Vs_train[mmsi], np.expand_dims(v_msg[:9], 0)), axis=0\n",
    "    )\n",
    "for key in tqdm(list(Vs_train.keys())):\n",
    "    Vs_train[key] = np.array(\n",
    "        sorted(Vs_train[key], key=lambda m_entry: m_entry[TIMESTAMP])\n",
    "    )\n",
    "\n",
    "# Validation set\n",
    "Vs_valid = dict()\n",
    "for v_msg in tqdm(aisdk_valid):\n",
    "    mmsi = int(v_msg[TRAJ_ID])\n",
    "    if not (mmsi in list(Vs_valid.keys())):\n",
    "        Vs_valid[mmsi] = np.empty((0, 9))\n",
    "    Vs_valid[mmsi] = np.concatenate(\n",
    "        (Vs_valid[mmsi], np.expand_dims(v_msg[:9], 0)), axis=0\n",
    "    )\n",
    "for key in tqdm(list(Vs_valid.keys())):\n",
    "    Vs_valid[key] = np.array(\n",
    "        sorted(Vs_valid[key], key=lambda m_entry: m_entry[TIMESTAMP])\n",
    "    )\n",
    "\n",
    "# Test set\n",
    "Vs_test = dict()\n",
    "for v_msg in tqdm(aisdk_test):\n",
    "    mmsi = int(v_msg[TRAJ_ID])\n",
    "    if not (mmsi in list(Vs_test.keys())):\n",
    "        Vs_test[mmsi] = np.empty((0, 9))\n",
    "    Vs_test[mmsi] = np.concatenate(\n",
    "        (Vs_test[mmsi], np.expand_dims(v_msg[:9], 0)), axis=0\n",
    "    )\n",
    "for key in tqdm(list(Vs_test.keys())):\n",
    "    Vs_test[key] = np.array(\n",
    "        sorted(Vs_test[key], key=lambda m_entry: m_entry[TIMESTAMP])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Normalisation for train data...\")\n",
    "for k in tqdm(list(Vs_train.keys())):\n",
    "    v = Vs_train[k]\n",
    "    v[:, LAT] = (v[:, LAT] - LAT_MIN) / (LAT_MAX - LAT_MIN)\n",
    "    v[:, LON] = (v[:, LON] - LON_MIN) / (LON_MAX - LON_MIN)\n",
    "    v[:, SOG][v[:, SOG] > SOG_MAX] = SOG_MAX\n",
    "    v[:, SOG] = v[:, SOG] / SOG_MAX\n",
    "    v[:, COG] = v[:, COG] / 360.0\n",
    "\n",
    "print(\"Normalisation for valid data...\")\n",
    "for k in tqdm(list(Vs_valid.keys())):\n",
    "    v = Vs_valid[k]\n",
    "    v[:, LAT] = (v[:, LAT] - LAT_MIN) / (LAT_MAX - LAT_MIN)\n",
    "    v[:, LON] = (v[:, LON] - LON_MIN) / (LON_MAX - LON_MIN)\n",
    "    v[:, SOG][v[:, SOG] > SOG_MAX] = SOG_MAX\n",
    "    v[:, SOG] = v[:, SOG] / SOG_MAX\n",
    "    v[:, COG] = v[:, COG] / 360.0\n",
    "\n",
    "print(\"Normalisation for test data...\")\n",
    "for k in tqdm(list(Vs_test.keys())):\n",
    "    v = Vs_test[k]\n",
    "    v[:, LAT] = (v[:, LAT] - LAT_MIN) / (LAT_MAX - LAT_MIN)\n",
    "    v[:, LON] = (v[:, LON] - LON_MIN) / (LON_MAX - LON_MIN)\n",
    "    v[:, SOG][v[:, SOG] > SOG_MAX] = SOG_MAX\n",
    "    v[:, SOG] = v[:, SOG] / SOG_MAX\n",
    "    v[:, COG] = v[:, COG] / 360.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename, filedict in zip(\n",
    "    [pkl_filename_train, pkl_filename_valid, pkl_filename_test],\n",
    "    [Vs_train, Vs_valid, Vs_test],\n",
    "):\n",
    "    print(\"Writing to\", os.path.join(out_path, filename))\n",
    "    with open(os.path.join(out_path, filename), \"wb\") as f:\n",
    "        pickle.dump(filedict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point forward, it is recommended to execute the code with the PY3GPU environment, as set up by Nguyen et al. (2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AISDK dataset\n",
    "LAT, LON, SOG, COG, NAME, SHIPTYPE, NAV_STT, TIMESTAMP, TRAJ_ID = list(range(9))\n",
    "\n",
    "# Pkl filenames\n",
    "pkl_filename_train = \"aisdk_20180208_train.pkl\"\n",
    "pkl_filename_valid = \"aisdk_20180208_valid.pkl\"\n",
    "pkl_filename_test = \"aisdk_20180208_test.pkl\"\n",
    "\n",
    "# Path to csv files\n",
    "data_path = \"../examples/data/aisdk_20180208_sample/\"\n",
    "csv_filename = \"aisdk_20180208_sample_20000.csv\"\n",
    "\n",
    "# Output path\n",
    "out_path = \"../examples/data/aisdk_20180208_sample/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate AIS mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input pipelines script for Tensorflow graph.\n",
    "This script is adapted from the original script of FIVO.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "dataset_path = os.path.join(data_path, pkl_filename_train)\n",
    "\n",
    "LAT_BINS = 100\n",
    "LON_BINS = 200\n",
    "SOG_BINS = 30\n",
    "COG_BINS = 72\n",
    "\n",
    "\n",
    "def sparse_AIS_to_dense(msgs_, num_timesteps, mmsis):\n",
    "    def create_dense_vect(msg, lat_bins=100, lon_bins=200, sog_bins=30, cog_bins=72):\n",
    "        lat, lon, sog, cog = msg[0], msg[1], msg[2], msg[3]\n",
    "        data_dim = lat_bins + lon_bins + sog_bins + cog_bins\n",
    "        dense_vect = np.zeros(data_dim)\n",
    "        dense_vect[int(lat * lat_bins)] = 1.0\n",
    "        dense_vect[int(lon * lon_bins) + lat_bins] = 1.0\n",
    "        dense_vect[int(sog * sog_bins) + lat_bins + lon_bins] = 1.0\n",
    "        dense_vect[int(cog * cog_bins) + lat_bins + lon_bins + sog_bins] = 1.0\n",
    "        return dense_vect\n",
    "\n",
    "    dense_msgs = []\n",
    "    for msg in msgs_:\n",
    "        dense_msgs.append(\n",
    "            create_dense_vect(\n",
    "                msg,\n",
    "                lat_bins=LAT_BINS,\n",
    "                lon_bins=LON_BINS,\n",
    "                sog_bins=SOG_BINS,\n",
    "                cog_bins=COG_BINS,\n",
    "            )\n",
    "        )\n",
    "    dense_msgs = np.array(dense_msgs)\n",
    "    return dense_msgs, num_timesteps, mmsis\n",
    "\n",
    "\n",
    "dirname = os.path.dirname(dataset_path)\n",
    "\n",
    "try:\n",
    "    with tf.gfile.Open(dataset_path, \"rb\") as f:\n",
    "        Vs = pickle.load(f)\n",
    "except:\n",
    "    with tf.gfile.Open(dataset_path, \"rb\") as f:\n",
    "        Vs = pickle.load(f, encoding=\"latin1\")\n",
    "\n",
    "data_dim = LAT_BINS + LON_BINS + SOG_BINS + COG_BINS\n",
    "\n",
    "mean_all = np.zeros((data_dim,))\n",
    "sum_all = np.zeros((data_dim,))\n",
    "total_ais_msg = 0\n",
    "\n",
    "current_mean = np.zeros((0, data_dim))\n",
    "current_ais_msg = 0\n",
    "\n",
    "count = 0\n",
    "for mmsi in list(Vs.keys()):\n",
    "    count += 1\n",
    "    print(count)\n",
    "    tmp = Vs[mmsi][:, [LAT, LON, SOG, COG]]\n",
    "    tmp[tmp == 1] = 0.99999\n",
    "    current_sparse_matrix, _, _ = sparse_AIS_to_dense(tmp, 0, 0)\n",
    "    #    current_mean = np.mean(current_sparse_matrix,axis = 0)\n",
    "    sum_all += np.sum(current_sparse_matrix, axis=0)\n",
    "    total_ais_msg += len(current_sparse_matrix)\n",
    "\n",
    "mean = sum_all / total_ais_msg\n",
    "\n",
    "print(\"Writing to\", os.path.join(dirname, \"/mean.pkl\"))\n",
    "with open(dirname + \"/mean.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mean, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i \"../mobiml/models/geotracknet.py\"\n",
    "mode = \"train\"\n",
    "dataset_dir = \"../data/aisdk_20180208_sample\"\n",
    "trainingset_name = \"aisdk_20180208_train.pkl\"\n",
    "testset_name = \"aisdk_20180208_valid.pkl\"\n",
    "lat_min = 57.0\n",
    "lat_max = 58.0\n",
    "lon_min = 11.0\n",
    "lon_max = 13.0\n",
    "latent_size = 100\n",
    "batch_size = 32\n",
    "num_samples = 16\n",
    "learning_rate = 0.0003"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PY3GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
