{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobiML Nautilus Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from mobiml.datasets import BrestAIS, TIMESTAMP, MOVER_ID, TRAJ_ID, PreprocessedBrestAIS\n",
    "from mobiml.preprocessing import TrajectorySubsampler, TrajectoryFilter, TrajectoryEnricher, TrajectorySplitter\n",
    "from mobiml.loaders import TemporalSplitter\n",
    "from mobiml.transforms import DeltaDatasetCreator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading Brest / Nari data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This dataset can be downloaded from: https://zenodo.org/record/1167595/files/%5BP1%5D%20AIS%20Data.zip?download=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ais = BrestAIS(r\"../examples/data/nari_dynamic.csv\", filter_mid=True, nrows=100000)\n",
    "ais.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_static = pd.read_csv(r\"../examples/data/nari_static.csv\")\n",
    "df_static = df_static.sort_values('t') \\\n",
    "    .dropna(subset=['shiptype']) \\\n",
    "    .drop_duplicates(subset=['sourcemmsi'], keep='last')[['sourcemmsi', 'shiptype']]\n",
    "df_static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsample trajectories with $\\Delta t_{min}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ais = TrajectorySubsampler(ais).subsample(min_dt_sec=10)\n",
    "\n",
    "print(f'[Subsampling] Dataset AIS Positions: {len(ais.df)}')\n",
    "print(f'{ais.df.sort_values(TIMESTAMP).groupby(MOVER_ID)[TIMESTAMP].diff().dt.total_seconds().describe().astype(str)=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop trajectories with fewer than $Points_{min}$ locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ais = TrajectoryFilter(ais).filter_min_pts(min_pts=20)\n",
    "\n",
    "print(f'[Trajectory Pruning] Dataset AIS Positions: {len(ais.df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-calculate speed and course over ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ais = TrajectoryEnricher(ais).add_speed(units=('nm','h'), overwrite=True)\n",
    "ais = TrajectoryEnricher(ais).add_direction(overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop speed outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ais = TrajectoryFilter(ais).filter_speed(min_speed=1, max_speed=50)\n",
    "print(f'[Speed Outliers] Dataset AIS Positions: {len(ais.df)}')\n",
    "print(f'{ais.df[\"speed\"].describe().round(5).astype(str)=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal segmentation / splitting trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from datetime import timedelta\n",
    "ais = TrajectorySplitter(ais).split(observation_gap=timedelta(minutes=30))\n",
    "ais = TrajectoryFilter(ais).filter_min_pts(min_pts=10)\n",
    "print(f'[Temporal Segmentation] Dataset AIS Positions: {len(ais.df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais.df.groupby([MOVER_ID, TRAJ_ID]).apply(len, include_groups=False).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais.df.to_csv(\n",
    "    os.path.join('data/nautilus_trajectories_preprocessed.csv'),\n",
    "    index=False, \n",
    "    header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais = PreprocessedBrestAIS('data/nautilus_trajectories_preprocessed.csv')\n",
    "ais.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal train/dev/test split\n",
    "\n",
    "50/25/25 (e.g., 3mos, ~1.5mos will be used for train and ~0.75mos will be used for validation and testing, respectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais = TemporalSplitter(ais).split()\n",
    "ais.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sanity Check #1;\\n\\t{ais.df.groupby([MOVER_ID, TRAJ_ID, 'split'])[TIMESTAMP].is_monotonic_increasing.all()=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create VRF training dataset \n",
    "\n",
    "Create delta dataset (with x, y, and t deltas) and split it into constant-length windows for ML model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_delta = DeltaDatasetCreator(ais).get_delta_dataset('split', njobs=4)\n",
    "traj_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_delta_windows = DeltaDatasetCreator(ais).get_windowed_dataset('split', njobs=4)\n",
    "traj_delta_windows.to_pickle('data/traj_delta_windows.pickle')\n",
    "traj_delta_windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from mobiml.models.vrf import VesselRouteForecasting, RMSELoss, train_model, vrf_evaluate_model_singlehead\n",
    "from mobiml.models.vrf_dataset import VRFDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create unified train/dev/test dataset(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_delta_windows = pd.read_pickle('data/traj_delta_windows.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_delta_windows = traj_delta_windows.xs(1, level=1).copy()\n",
    "dev_delta_windows = traj_delta_windows.xs(2, level=1).copy()\n",
    "test_delta_windows = traj_delta_windows.xs(3, level=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_delta_windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create kinematic features' temporal sequence (i.e. training dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS=1\n",
    "\n",
    "train_dataset = VRFDataset(train_delta_windows)\n",
    "dev_dataset, test_dataset = VRFDataset(dev_delta_windows, scaler=train_dataset.scaler),\\\n",
    "                            VRFDataset(test_delta_windows, scaler=train_dataset.scaler)\n",
    "\n",
    "train_loader, dev_loader, test_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True, collate_fn=train_dataset.pad_collate),\\\n",
    "                                        DataLoader(dev_dataset,   batch_size=BS, shuffle=False, collate_fn=dev_dataset.pad_collate),\\\n",
    "                                        DataLoader(test_dataset,  batch_size=BS, shuffle=False, collate_fn=test_dataset.pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "GPUID=0\n",
    "\n",
    "device = torch.device(f'cuda:{GPUID}') if torch.cuda.is_available() else torch.device('cpu')\n",
    "ddc = DeltaDatasetCreator(None)\n",
    "\n",
    "\n",
    "model_params = dict({},\n",
    "    input_size=len(ddc.input_feats),\n",
    "    scale=dict(\n",
    "        sigma=torch.Tensor(train_dataset.scaler.scale_[:2]), \n",
    "        mu=torch.Tensor(train_dataset.scaler.mean_[:2])\n",
    "    ),\n",
    "    bidirectional=True,\n",
    "    num_layers=1,\n",
    "    hidden_size=350,\n",
    "    fc_layers=[150,]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = VesselRouteForecasting(**model_params)\n",
    "model.to(device)\n",
    "\n",
    "print(model)\n",
    "print(f'{device=}')\n",
    "\n",
    "criterion = RMSELoss(eps=1e-4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_DT = 1800\n",
    "\n",
    "PATIENCE = 0  # original: 10 may be why the best model was not saved since we're only doing 3 training rounds in this demo\n",
    "model_path =   f'data/nautilus.model'\n",
    "\n",
    "evaluate_fun_params = dict(\n",
    "    bins=np.arange(0, MAX_DT+1, 300)\n",
    ")\n",
    "\n",
    "early_stop_params = dict(\n",
    "    patience=PATIENCE,\n",
    "    save_best=True,\n",
    "    path=model_path\n",
    ")\n",
    "\n",
    "save_current_params = dict(\n",
    "    path=model_path\n",
    ")\n",
    "\n",
    "train_model(\n",
    "    model, device, criterion, optimizer, 10, # originally: 100 rounds\n",
    "    train_loader, dev_loader, early_stop=True, save_current=True, \n",
    "    evaluate_fun=vrf_evaluate_model_singlehead, evaluate_fun_params=evaluate_fun_params,\n",
    "    early_stop_params=early_stop_params, save_current_params=save_current_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(model_path)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "vrf_evaluate_model_singlehead(model, device, criterion, test_loader, desc='ADE @ Test Set...', **evaluate_fun_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movingml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
